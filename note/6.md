## 6.2 Locality

Well-written computer program tend to exhibit good locality. That is, they tend to reference data items that are near other recently referenced data items or that were recently referenced themselves. This tendency is known as the *principle of locality*. 

```c++
int sumvec(int v[N]) {
	int i, sum = 0;
	for(i = 0; i < N; i++)
		sum += v[i];
	return sum
}
```

A function such as `sumvec` that visits each element of a vector sequentially is said to have a *stride-1 reference pattern*. In general, as the stride increase, the spatial locality decrease. 

## 6.3 The Memory Hierarchy

In general, a *cache* is a small, fast storage device that acts as a staging area for the data objects stored in a larger, slower devices. 

<img src = "C:\Users\16549\AppData\Roaming\Typora\typora-user-images\image-20240919171528615.png" width = 500>

When the program needs a particular data object $d$ from level $k+1$, it first looks for $d$ at level $k$. If $d$ happens to be cached at level $k$, then this is called *cache hit*. 

If, on the other hand,  the data object $d$ is not cached at level $k$, then what we had is called a *cache miss*. This process of overwriting an existing block is known as *replacing* or *evicting* the block. 

Memory hierarchy work because programs tend to exhibit locality: 

- *Exploiting temporal locality*. Once a data object has been copied into the cache on the first miss, we can expect a number of subsequent hits on that object. 
- *Exploiting spatial locality*. We can expect that the cost of copying a block after a miss will be amortized by subsequent references to other objects within that block. 

## 6.4 Cache Memory

<img src = "C:\Users\16549\AppData\Roaming\Typora\typora-user-images\image-20240919174118873.png" width = "400">

Consider a computer system where each memory address has $m$ bits that form $M = 2^m$ unique address. A cache for such a machine is organized as an array of $S = 2^s$ *cache sets*. Each set consists $E$ cache lines. Each line consists of a data *block* of $B = 2^b$ bytes, a *valid bit* that indicates whether the line has the meaningful information, and $t = m-(b+s)$ *tag bits*. 

The concatenation of the tag and index bits uniquely identifies each block in memory. Blocks that map to the same cache set are uniquely identifies by the tag. 

A line in the set contains the word if and only if the valid bit is set and the tag bits in the line match the tag bits in the address. 

If the word requested by the CPU is not stored in any of the lines in the set, then the cache must fetch the block that contains the word in memory. Some policies draw on the principle of locality to try to minimize the probability that the replaced line will be referenced in the near future.

